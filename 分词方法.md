## Summary

è‹±æ–‡åˆ†è¯æœ‰ä¸‰ç§ç²’åº¦ï¼šwordã€charã€subword (token)

- wordåˆ†è¯çš„å¥½å¤„æ˜¯å®Œç¾å¥‘åˆäººç±»æ­£å¸¸ç†è§£ä¸€ä¸ªè¯çš„å«ä¹‰ï¼Œä½†æœ‰å¾ˆå¤šç¼ºç‚¹
  - è¯å…¸ä¼šéå¸¸åºå¤§ï¼Œä¸”å®¹æ˜“å‡ºç°OOVé—®é¢˜
  - ä½é¢‘è¯æ±‡æ— æ³•å¾—åˆ°å¾ˆå¥½çš„è®­ç»ƒ
- charåˆ†è¯åªéœ€è¦26ä¸ªè‹±æ–‡å­—æ¯å°±èƒ½è¡¨ç¤ºæ‰€æœ‰è‹±æ–‡å•è¯ï¼Œä½†è¿™æ ·
  - æ¯ä¸ªå­—æ¯éœ€è¦å®¹çº³éå¸¸å¤šçš„è¯­ä¹‰ä¿¡æ¯ï¼Œéš¾ä»¥è®­ç»ƒ
  - è¯æ±‡çš„è¾¹ç•Œä¿¡æ¯ä¸¥é‡ä¸¢å¤±
  - æ¨¡å‹è¾“å…¥å¤ªé•¿
- subwordåˆ†è¯æ˜¯ç›¸å¯¹è¾ƒä¼˜çš„æ–¹æ¡ˆï¼Œç»¼åˆäº†ä¸Šè¿°ä¸¤è€…ä¼˜ç‚¹ï¼Œå½“ç„¶ä¹Ÿå°±ç»¼åˆäº†ç¼ºç‚¹

## BPE (Byte Pair Encoding)

[Byte-Pair Encoding tokenization - Hugging Face NLP Course](https://huggingface.co/learn/nlp-course/en/chapter6/5?fw=pt)

> **Used in GPT1ã€GPT2ã€GPT3ã€RoBERTaã€BARTã€DeBERTa**
>
> The **LLaMA** tokenizer is a BPE model based on sentencepiece.

> The GPT-2 and RoBERTa tokenizers (which are pretty similar) have a clever way to deal with this: they donâ€™t look at words as being written with Unicode characters, but with bytes. This way the base vocabulary has a small size (256), but every character you can think of will still be included and not end up being converted to the unknown token. This trick is called ***byte-level BPE***.

### Training algorithm

å‡è®¾é¢„è®­ç»ƒè¯­æ–™åº“å¦‚ä¸‹:

```
("hug", 10), ("pug", 5), ("pun", 12), ("bun", 4), ("hugs", 5)
```

("hug", 10)è¡¨ç¤º"hug"åœ¨è¯­æ–™åº“å‡ºç°äº†10æ¬¡.

1. å»ºç«‹åˆå§‹Vocabulary, åŒ…å«ç”¨äºè¡¨ç¤ºè¿™äº›å•è¯çš„æ‰€æœ‰å­—ç¬¦

   ```
   Vocabulary: ["b", "g", "h", "n", "p", "s", "u"]
   ```

   > The GPT-2 and RoBERTa tokenizers (which are pretty similar) have a clever way to deal with this: they donâ€™t look at words as being written with Unicode characters, but with bytes. This way the base vocabulary has a small size (256), but every character you can think of will still be included and not end up being converted to the unknown token. This trick is called *byte-level BPE*.

2. å°†Corpusä¸­çš„å•è¯æŒ‰ç…§Vocabularyæ‹†åˆ†æˆtoken.

   ```
   Corpus: ("h" "u" "g", 10), ("p" "u" "g", 5), ("p" "u" "n", 12), ("b" "u" "n", 4), ("h" "u" "g" "s", 5)
   ```

3. æ¯ä¸€æ­¥, æ‰¾åˆ°æœ€å‡ºç°é¢‘ç‡æœ€é«˜çš„pair(pairå®šä¹‰ä¸ºä¸€ä¸ªwordä¸­è¿ç»­çš„ä¸¤ä¸ªtoken), å°†å®ƒä»¬åˆå¹¶æˆä¸€ä¸ªæ–°çš„tokenå¹¶æ·»åŠ åˆ°Vocabulary, åŒæ—¶å°†Corpusä¸­çš„æ‰€æœ‰è¿™ç§pairåˆå¹¶. æˆ‘ä»¬å°†è¿™ä¸ªpairçš„åˆå¹¶å®šä¹‰ä¸€ä¸ªmerge

   ``` 
   merges = []
   
   step 1:
   æœ€é«˜é¢‘pair: ("u", "g"), 20æ¬¡, åˆ†åˆ«å‡ºç°åœ¨"hug", "pug"å’Œ"hugs"ä¸­
   Vocabulary: ["b", "g", "h", "n", "p", "s", "u", "ug"]
   Corpus: ("h" "ug", 10), ("p" "ug", 5), ("p" "u" "n", 12), ("b" "u" "n", 4), ("h" "ug" "s", 5)
   merges = {("u", "g"): "ug", }
   
   step 2:
   æœ€é«˜é¢‘pair: ("u", "n"), 16æ¬¡
   Vocabulary: ["b", "g", "h", "n", "p", "s", "u", "ug", "un"]
   Corpus: ("h" "ug", 10), ("p" "ug", 5), ("p" "un", 12), ("b" "un", 4), ("h" "ug" "s", 5)
   merges = {("u", "g"): "ug", ("u", "n"): "un"}
   
   step 3:
   æœ€é«˜é¢‘pair: ("h", "ug")
   Vocabulary: ["b", "g", "h", "n", "p", "s", "u", "ug", "un", "hug"]
   Corpus: ("hug", 10), ("p" "ug", 5), ("p" "un", 12), ("b" "un", 4), ("hug" "s", 5)
   merges = {("u", "g"): "ug", ("u", "n"): "un", ("h", "ug"): "hug"}
   
   ...
   ```

4. é‡å¤ç›´åˆ°vocabulary sizeåˆ°è¾¾æŒ‡å®šå¤§å°, æˆ‘ä»¬å¾—åˆ°æœ€ç»ˆçš„merges

### Tokenization algorithm

å¾—åˆ°mergeså, éå†æ‰€æœ‰çš„merges, å¯¹wordä¸­çš„characterè¿›è¡Œåˆå¹¶.

```python
def tokenize(text):
    pre_tokenize_result = tokenizer._tokenizer.pre_tokenizer.pre_tokenize_str(text)
    pre_tokenized_text = [word for word, offset in pre_tokenize_result]
    splits = [[l for l in word] for word in pre_tokenized_text]
    for pair, merge in merges.items():
        for idx, split in enumerate(splits):
            i = 0
            while i < len(split) - 1:
                if split[i] == pair[0] and split[i + 1] == pair[1]:
                    split = split[:i] + [merge] + split[i + 2 :]
                else:
                    i += 1
            splits[idx] = split

    return sum(splits, [])
```



ä¾‹å¦‚å¯¹äº`merges = {("u", "g"): "ug", ("u", "n"): "un", ("h", "ug"): "hug"}`, æœ‰

```
"bug" -> ["b", "u", "g"] -> ["b", "ug"]
"thug" -> ["t", "h", "u", "g"] -> ["UNK", "h", "ug"] -> ["UNK", "hug"]
```

## WordPiece

[WordPiece tokenization - Hugging Face NLP Course](https://huggingface.co/learn/nlp-course/en/chapter6/6?fw=pt)

> WordPiece is the subword tokenization algorithm used for **[BERT](https://huggingface.co/docs/transformers/en/model_doc/bert), [DistilBERT](https://huggingface.co/docs/transformers/en/model_doc/distilbert), and [Electra](https://huggingface.co/docs/transformers/en/model_doc/electra)**. 

### Training algorithm

WordPieceæ•´ä½“ç®—æ³•æµç¨‹ä¸BPEå¾ˆåƒ, ä¸è¿‡æœ‰äº›ç»†å¾®çš„å·®å¼‚

1. WordPieceç”¨ prefix (like `##` for BERT)åŒºåˆ†äº†å‡ºç°åœ¨wordå¼€å¤´çš„characterå’Œå‡ºç°åœ¨wordå†…éƒ¨çš„character. ä¾‹å¦‚: `word`ä¼šè¢«splitä¸º`w ##o ##r ##d`. å› æ­¤, ä»¥BPEä¸­çš„ä¸¾çš„é¢„è®­ç»ƒè¯­æ–™åº“ä¸ºä¾‹, åˆå§‹çš„Vocabularyå’ŒCorpusä¸º: 

   ```
   Vocabulary: Vocabulary: ["b", "h", "p", "##g", "##n", "##s", "##u"]
   Corpus: ("h" "##u" "##g", 10), ("p" "##u" "##g", 5), ("p" "##u" "##n", 12), ("b" "##u" "##n", 4), ("h" "##u" "##g" "##s", 5)
   ```

2. WordPieceä¸ä¼šé€‰æ‹©å‡ºç°é¢‘ç‡æœ€é«˜çš„pairï¼Œè€Œæ˜¯ä½¿ç”¨ä»¥ä¸‹å…¬å¼ä¸ºæ¯å¯¹pairè®¡ç®—ä¸€ä¸ªåˆ†æ•°:

   ```
   score=(freq_of_pair)/(freq_of_first_elementÃ—freq_of_second_element)
   ```

   å¯¹äºä¾‹å­æ¥è¯´, å‡ºç°é¢‘ç‡æœ€é«˜çš„pairä¸º("##u", "##g"), ä½†å•ç‹¬"##u"å‡ºç°çš„é¢‘ç‡ä¹Ÿéå¸¸é«˜, æ‰€ä»¥å…¶æœ€ç»ˆå¾—åˆ†1/36. å®é™…ä¸Šå¾—åˆ†æœ€é«˜æ˜¯("##g", "##s"), ä¸º1/20. æ‰€ä»¥step 1å­¦ä¹ åˆ°çš„mergeæ˜¯`("##g", "##s") -> ("##gs")`, step 2çš„mergeæ˜¯`("h", "##u") -> "hu"`, step 3çš„mergeä¸º `("hu", "##g") -> "hug"`. 3æ­¥åçš„Vocabularyå’ŒCorpusä¸º: 

   ```
   Vocabulary: ["b", "h", "p", "##g", "##n", "##s", "##u", "##gs", "hu", "hug"]
   Corpus: ("hug", 10), ("p" "##u" "##g", 5), ("p" "##u" "##n", 12), ("b" "##u" "##n", 4), ("hu" "##gs", 5)
   ```

### Tokenization algorithm

ä¸åŒäºBPE, WordPieceåªä¿å­˜æœ€ç»ˆçš„Vocabulary, è€Œä¸æ˜¯ä¿å­˜merges. ä¸”WordPieceä¼˜å…ˆæŒ‰Vocabularyä¸­æœ€é•¿çš„subwordè¿›è¡Œåˆ†è¯.

```python
def encode_word(word):
    tokens = []
    while len(word) > 0:
        i = len(word)
        while i > 0 and word[:i] not in vocab:
            i -= 1
        if i == 0:
            return ["[UNK]"]
        tokens.append(word[:i])
        word = word[i:]
        if len(word) > 0:
            word = f"##{word}"
    return tokens

def tokenize(text):
    pre_tokenize_result = tokenizer._tokenizer.pre_tokenizer.pre_tokenize_str(text)
    pre_tokenized_text = [word for word, offset in pre_tokenize_result]
    encoded_words = [encode_word(word) for word in pre_tokenized_text]
    return sum(encoded_words, [])
```

å¯¹äºä¾‹å­ä¸­å¾—åˆ°çš„Vocabulary, WordPieceçš„åˆ†è¯ç»“æœå¦‚ä¸‹:

```
"hugs" -> ["hug", "##s"]
"bugs" -> ["b", "##ugs"] -> ["b", "##u, "##gs"]
```

## Unigram

[Unigram tokenization - Hugging Face NLP Course](https://huggingface.co/learn/nlp-course/en/chapter6/7?fw=pt)

### Training algorithm

ä¸åŒäºWordPieceå’ŒBPEé€æ­¥æ‰©å…… Vocabulary, Unigramçš„åˆå§‹ Vocabulary å¾ˆå¤§, ç„¶åé€æ­¥åˆ å‡.

æœ‰å¾ˆå¤šæ–¹æ³•ç”¨äºåˆ›å»ºåˆå§‹ Vocabulary, ä¾‹å¦‚: pre-tokenized words ä¸­çš„æœ€å¸¸è§å­ä¸²(most common substrings), æˆ–è€…è°ƒç”¨BPE,è®¾ç½®ä¸€ä¸ªå¾ˆå¤§çš„vocabulary size.

1. æ¯ä¸€æ­¥, è®¡ç®—ç»™å®šçš„å½“å‰ Vocabulary ä¸‹, æ•´ä¸ª corpus çš„ loss. 
2. ç„¶åå¯¹äº vocabulary ä¸­çš„æ¯ä¸ª token, è®¡ç®—å½“å®ƒä» vocabulary ç§»é™¤å, ä¼šå¯¼è‡´ loss å¢åŠ å¤šå°‘. 
3. æ‰¾åˆ°ä½¿ loss å¢åŠ æœ€å°‘çš„tokens, è¿™äº›ç¬¦å·å¯¹è¯­æ–™åº“æ•´ä½“æŸå¤±çš„å½±å“è¾ƒå°ï¼Œå› æ­¤ä»æŸç§æ„ä¹‰ä¸Šè¯´ï¼Œå®ƒä»¬â€œä¸å¤ªéœ€è¦â€ï¼Œæ˜¯åˆ é™¤çš„æœ€ä½³å€™é€‰ã€‚

è¿™æ˜¯ä¸€ä¸ªéå¸¸è€—æ—¶çš„è¿‡ç¨‹, å› æ­¤ä¸€èˆ¬æ¯æ¬¡éƒ½åˆ æ‰ 10% æˆ– 20% çš„tokens. æ­¤å¤–ä¸ºäº†ä¿è¯æ‰€æœ‰çš„ word éƒ½èƒ½è¢« tokenized, æ‰€æœ‰çš„ base character éƒ½ä¸ä¼šè¢«åˆ é™¤.

ä¾ç„¶å‡è®¾ Corpus å¦‚ä¸‹: 

```
("hug", 10), ("pug", 5), ("pun", 12), ("bun", 4), ("hugs", 5)
```

å¯¹äºè¿™ä¸ªä¾‹å­, æˆ‘ä»¬ç”¨æ‰€æœ‰çš„ substring åˆå§‹åŒ– Vocabulary:

```
["h", "u", "g", "hu", "ug", "p", "pu", "n", "un", "b", "bu", "s", "hug", "gs", "ugs"]
```

æ€ä¹ˆç®— loss å‘¢, æš‚ä¸”æŒ‰ä¸‹ä¸è¡¨.

### Tokenization algorithm

Unigram model, ä¸€ç§è¯­è¨€æ¨¡å‹, æ¯ä¸ªtokenä¸å‰æ–‡æ˜¯ç‹¬ç«‹çš„, å³ $P(x_i)=P(x_i|x_1, \dots. x_{i-1})$. å¦‚æœç”¨ä¸€ä¸ª Unigram language model æ¥ç”Ÿæˆæ–‡æœ¬, å®ƒå°†æ€»æ˜¯é¢„æµ‹the most common token. 

ä¸€ä¸ª token çš„æ¦‚ç‡ $P(x)$, ç­‰äºå®ƒåœ¨åŸå§‹ corpus ä¸­å‡ºç°çš„é¢‘ç‡é™¤ä»¥æ‰€æœ‰ token çš„é¢‘ç‡çš„å’Œ.

ä¸‹é¢æ˜¯æ‰€æœ‰ è¯è¡¨ä¸­æ‰€æœ‰ substring çš„é¢‘ç‡: 

```
("h", 15) ("u", 36) ("g", 20) ("hu", 15) ("ug", 20) ("p", 17) ("pu", 17) ("n", 16)
("un", 16) ("b", 4) ("bu", 4) ("s", 5) ("hug", 15) ("gs", 5) ("ugs", 5)
```

æ‰€æœ‰ token çš„é¢‘ç‡çš„å’Œä¸º 210, "ug"çš„é¢‘ç‡ä¸º 20, é‚£ä¹ˆ $P(\text{'ug'})=\frac{20}{210}$.

tokenize ä¸€ä¸ª word çš„  æ­¥éª¤å¦‚ä¸‹: 

1. æ‰¾åˆ°æ‰€æœ‰å¯èƒ½çš„åˆ†å‰²æ–¹å¼(segmentations), ä¾‹å¦‚, pug å¯ä»¥åˆ†ä¸º, `["p", "u", "g"], ["pu", "g"], ["p", "ug"]`

2. è®¡ç®—æ¯ä¸ªæ¯ç§åˆ†å‰²æ–¹å¼çš„æ¦‚ç‡, å› ä¸ºæ¯ä¸ªtokenéƒ½æ˜¯ç‹¬ç«‹çš„, å› æ­¤è¿™ä¸ªæ¦‚ç‡å°±æ˜¯å„ä¸ªtokenæ¦‚ç‡çš„ä¹˜ç§¯
   $$
   P([\text{'p'}, \text{'u'}, \text{'g'}]) = P(\text{'p'}) \times P(\text{'u'}) \times P(\text{'g'}) = \frac{5}{210} \times \frac{36}{210} \times \frac{20}{210} = 0.000389
   $$

   ```
   ["p", "u", "g"] : 0.000389
   ["p", "ug"] : 0.0022676
   ["pu", "g"] : 0.0022676
   ```

3. é€‰æ‹©æ¦‚ç‡æœ€é«˜çš„åˆ†è¯æ–¹å¼ (ä¸€èˆ¬æ¥è¯´ï¼Œä½¿ç”¨å°½å¯èƒ½å°‘çš„ tokens å°†å…·æœ‰æœ€é«˜çš„æ¦‚ç‡(å› ä¸ºæ¯ä¸ª token é‡å¤é™¤ä»¥210)ï¼Œè¿™ä¸æˆ‘ä»¬ç›´è§‚åœ°æƒ³è¦çš„ç›¸å¯¹åº”: å°†ä¸€ä¸ªå•è¯åˆ†æˆå°½å¯èƒ½å°‘çš„tokens)

> åœ¨è¿™ä¸ªä¾‹å­ä¸­, æ‰¾åˆ°æ‰€æœ‰çš„ segmentation å¹¶è®¡ç®—å®ƒä»¬çš„æ¦‚ç‡æ˜¯ç®€å•çš„. ä½†ä¸€èˆ¬æƒ…å†µä¸‹, è¿™æ˜¯æœ‰äº›éš¾ä»¥å®ç°çš„. æœ‰ä¸€ä¸ªå¤„ç†è¯¥é—®é¢˜çš„ç»å…¸ç®—æ³•, å«*Viterbi algorithm*. 

### Back to training

loss çš„è®¡ç®—å°±æ˜¯, ç”¨å½“å‰çš„Unigram model(ä¸vocabularyæœ‰å…³)å¯¹corpusä¸­çš„wordè¿›è¡Œtokenization, å¾—åˆ°å¯¹åº”çš„score, losså°±æ˜¯è¿™äº›scoreçš„ negative log likelihood, å³corpusä¸­æ‰€æœ‰wordçš„ $-\log (\text{score})$ çš„å’Œ.

å¯¹corpusä¸­wordçš„tokenizationç»“æœå¦‚ä¸‹: 

```
"hug": ["hug"] (score 0.071428)
"pug": ["pu", "g"] (score 0.007710)
"pun": ["pu", "n"] (score 0.006168)
"bun": ["bu", "n"] (score 0.001451)
"hugs": ["hug", "s"] (score 0.001701)
```

åˆ™lossä¸º: 

```
10 * (-log(0.071428)) + 5 * (-log(0.007710)) + 12 * (-log(0.006168)) + 4 * (-log(0.001451)) + 5 * (-log(0.001701)) = 169.8
```



> Unigram is a subword tokenization algorithm introduced in [Subword Regularization: Improving Neural Network Translation Models with Multiple Subword Candidates (Kudo, 2018)](https://arxiv.org/pdf/1804.10959.pdf). <u>In contrast to BPE or WordPiece, Unigram initializes its base vocabulary to a large number of symbols and progressively trims down each symbol to obtain a smaller vocabulary.</u> The base vocabulary could for instance correspond to all pre-tokenized words and the most common substrings. Unigram is not used directly for any of the models in the transformers, but itâ€™s used in conjunction with SentencePiece.
>
> At each training step, the Unigram algorithm defines a loss (often defined as the log-likelihood) over the training data given the current vocabulary and a unigram language model. Then, for each symbol in the vocabulary, the algorithm computes how much the overall loss would increase if the symbol was to be removed from the vocabulary. Unigram then removes p (with p usually being 10% or 20%) percent of the symbols whose loss increase is the lowest, *i.e.* those symbols that least affect the overall loss over the training data. This process is repeated until the vocabulary has reached the desired size. The Unigram algorithm always keeps the base characters so that any word can be tokenized.
>
> Because Unigram is not based on merge rules (in contrast to BPE and WordPiece), the algorithm has several ways of tokenizing new text after training. As an example, if a trained Unigram tokenizer exhibits the vocabulary:
>
> ```
> ["b", "g", "h", "n", "p", "s", "u", "ug", "un", "hug"],
> ```
>
> `"hugs"` could be tokenized both as `["hug", "s"]`, `["h", "ug", "s"]` or `["h", "u", "g", "s"]`. So which one to choose? Unigram saves the probability of each token in the training corpus on top of saving the vocabulary so that the probability of each possible tokenization can be computed after training. The algorithm simply picks the most likely tokenization in practice, but also offers the possibility to sample a possible tokenization according to their probabilities.

## SentencePiece

[Summary of the tokenizers (huggingface.co)](https://huggingface.co/docs/transformers/en/tokenizer_summary#sentencepiece)

ä¸æ˜¯æ‰€æœ‰çš„è¯­è¨€éƒ½æ˜¯ç”¨ç©ºæ ¼åˆ†éš”, sentencePieceå¯ä»¥è§£å†³è¿™ä¸ªé—®é¢˜.

pre-tokenization: It considers the text as a sequence of Unicode characters, and replaces spaces with a special character, `â–`. ä»è¿™é‡Œä¹Ÿå¯ä»¥çœ‹å‡ºå®ƒä¸ä¾èµ–ç©ºæ ¼åšpre-tokenization, äº‹å®ä¸Šå°±ç®—æœ‰ç©ºæ ¼, ä»–ä¹Ÿä¼šæŠŠç©ºæ ¼å¤„ç†æ‰.

> All tokenization algorithms described so far have the same problem: <u>It is assumed that the input text uses spaces to separate words. However, not all languages use spaces to separate words</u>. One possible solution is to use language specific pre-tokenizers, *e.g.* XLM uses a specific Chinese, Japanese, and Thai pre-tokenizer). To solve this problem more generally, [SentencePiece: A simple and language independent subword tokenizer and detokenizer for Neural Text Processing (Kudo et al., 2018)](https://arxiv.org/pdf/1808.06226.pdf) treats the input as a raw input stream, thus including the space in the set of characters to use. <u>It then uses the BPE or unigram algorithm to construct the appropriate vocabulary.</u>
>
> Used in conjunction with the Unigram algorithm (see [section 7](https://huggingface.co/course/chapter7/7)), it doesnâ€™t even require a pre-tokenization step, which is very useful for languages where the space character is not used (like Chinese or Japanese).
>
> ```python
> from transformers import XLNetTokenizer
> 
> tokenizer = XLNetTokenizer.from_pretrained("xlnet/xlnet-base-cased")
> tokenizer.tokenize("Don't you love ğŸ¤— Transformers? We sure do.")
> ["â–Don", "'", "t", "â–you", "â–love", "â–", "ğŸ¤—", "â–", "Transform", "ers", "?", "â–We", "â–sure", "â–do", "."]
> ```
>
> The XLNetTokenizer uses SentencePiece for example, which is also why in the example earlier the `"â–"` character was included in the vocabulary. Decoding with SentencePiece is very easy since all tokens can just be concatenated and `"â–"` is replaced by a space.
>
> **All transformers models in the library that use SentencePiece use it in combination with unigram. Examples of models using SentencePiece are [ALBERT](https://huggingface.co/docs/transformers/en/model_doc/albert), [XLNet](https://huggingface.co/docs/transformers/en/model_doc/xlnet), [Marian](https://huggingface.co/docs/transformers/en/model_doc/marian), and [T5](https://huggingface.co/docs/transformers/en/model_doc/t5).**
