## Summary

英文分词有三种粒度：word、char、subword (token)

- word分词的好处是完美契合人类正常理解一个词的含义，但有很多缺点
  - 词典会非常庞大，且容易出现OOV问题
  - 低频词汇无法得到很好的训练
- char分词只需要26个英文字母就能表示所有英文单词，但这样
  - 每个字母需要容纳非常多的语义信息，难以训练
  - 词汇的边界信息严重丢失
  - 模型输入太长
- subword分词是相对较优的方案，综合了上述两者优点，当然也就综合了缺点

## BPE (Byte Pair Encoding)

[Byte-Pair Encoding tokenization - Hugging Face NLP Course](https://huggingface.co/learn/nlp-course/en/chapter6/5?fw=pt)

> **Used in GPT1、GPT2、GPT3、RoBERTa、BART、DeBERTa**
>
> The **LLaMA** tokenizer is a BPE model based on sentencepiece.

> The GPT-2 and RoBERTa tokenizers (which are pretty similar) have a clever way to deal with this: they don’t look at words as being written with Unicode characters, but with bytes. This way the base vocabulary has a small size (256), but every character you can think of will still be included and not end up being converted to the unknown token. This trick is called ***byte-level BPE***.

### Training algorithm

假设预训练语料库如下:

```
("hug", 10), ("pug", 5), ("pun", 12), ("bun", 4), ("hugs", 5)
```

("hug", 10)表示"hug"在语料库出现了10次.

1. 建立初始Vocabulary, 包含用于表示这些单词的所有字符

   ```
   Vocabulary: ["b", "g", "h", "n", "p", "s", "u"]
   ```

   > The GPT-2 and RoBERTa tokenizers (which are pretty similar) have a clever way to deal with this: they don’t look at words as being written with Unicode characters, but with bytes. This way the base vocabulary has a small size (256), but every character you can think of will still be included and not end up being converted to the unknown token. This trick is called *byte-level BPE*.

2. 将Corpus中的单词按照Vocabulary拆分成token.

   ```
   Corpus: ("h" "u" "g", 10), ("p" "u" "g", 5), ("p" "u" "n", 12), ("b" "u" "n", 4), ("h" "u" "g" "s", 5)
   ```

3. 每一步, 找到最出现频率最高的pair(pair定义为一个word中连续的两个token), 将它们合并成一个新的token并添加到Vocabulary, 同时将Corpus中的所有这种pair合并. 我们将这个pair的合并定义一个merge

   ``` 
   merges = []
   
   step 1:
   最高频pair: ("u", "g"), 20次, 分别出现在"hug", "pug"和"hugs"中
   Vocabulary: ["b", "g", "h", "n", "p", "s", "u", "ug"]
   Corpus: ("h" "ug", 10), ("p" "ug", 5), ("p" "u" "n", 12), ("b" "u" "n", 4), ("h" "ug" "s", 5)
   merges = {("u", "g"): "ug", }
   
   step 2:
   最高频pair: ("u", "n"), 16次
   Vocabulary: ["b", "g", "h", "n", "p", "s", "u", "ug", "un"]
   Corpus: ("h" "ug", 10), ("p" "ug", 5), ("p" "un", 12), ("b" "un", 4), ("h" "ug" "s", 5)
   merges = {("u", "g"): "ug", ("u", "n"): "un"}
   
   step 3:
   最高频pair: ("h", "ug")
   Vocabulary: ["b", "g", "h", "n", "p", "s", "u", "ug", "un", "hug"]
   Corpus: ("hug", 10), ("p" "ug", 5), ("p" "un", 12), ("b" "un", 4), ("hug" "s", 5)
   merges = {("u", "g"): "ug", ("u", "n"): "un", ("h", "ug"): "hug"}
   
   ...
   ```

4. 重复直到vocabulary size到达指定大小, 我们得到最终的merges

### Tokenization algorithm

得到merges后, 遍历所有的merges, 对word中的character进行合并.

```python
def tokenize(text):
    pre_tokenize_result = tokenizer._tokenizer.pre_tokenizer.pre_tokenize_str(text)
    pre_tokenized_text = [word for word, offset in pre_tokenize_result]
    splits = [[l for l in word] for word in pre_tokenized_text]
    for pair, merge in merges.items():
        for idx, split in enumerate(splits):
            i = 0
            while i < len(split) - 1:
                if split[i] == pair[0] and split[i + 1] == pair[1]:
                    split = split[:i] + [merge] + split[i + 2 :]
                else:
                    i += 1
            splits[idx] = split

    return sum(splits, [])
```



例如对于`merges = {("u", "g"): "ug", ("u", "n"): "un", ("h", "ug"): "hug"}`, 有

```
"bug" -> ["b", "u", "g"] -> ["b", "ug"]
"thug" -> ["t", "h", "u", "g"] -> ["UNK", "h", "ug"] -> ["UNK", "hug"]
```

## WordPiece

[WordPiece tokenization - Hugging Face NLP Course](https://huggingface.co/learn/nlp-course/en/chapter6/6?fw=pt)

> WordPiece is the subword tokenization algorithm used for **[BERT](https://huggingface.co/docs/transformers/en/model_doc/bert), [DistilBERT](https://huggingface.co/docs/transformers/en/model_doc/distilbert), and [Electra](https://huggingface.co/docs/transformers/en/model_doc/electra)**. 

### Training algorithm

WordPiece整体算法流程与BPE很像, 不过有些细微的差异

1. WordPiece用 prefix (like `##` for BERT)区分了出现在word开头的character和出现在word内部的character. 例如: `word`会被split为`w ##o ##r ##d`. 因此, 以BPE中的举的预训练语料库为例, 初始的Vocabulary和Corpus为: 

   ```
   Vocabulary: Vocabulary: ["b", "h", "p", "##g", "##n", "##s", "##u"]
   Corpus: ("h" "##u" "##g", 10), ("p" "##u" "##g", 5), ("p" "##u" "##n", 12), ("b" "##u" "##n", 4), ("h" "##u" "##g" "##s", 5)
   ```

2. WordPiece不会选择出现频率最高的pair，而是使用以下公式为每对pair计算一个分数:

   ```
   score=(freq_of_pair)/(freq_of_first_element×freq_of_second_element)
   ```

   对于例子来说, 出现频率最高的pair为("##u", "##g"), 但单独"##u"出现的频率也非常高, 所以其最终得分1/36. 实际上得分最高是("##g", "##s"), 为1/20. 所以step 1学习到的merge是`("##g", "##s") -> ("##gs")`, step 2的merge是`("h", "##u") -> "hu"`, step 3的merge为 `("hu", "##g") -> "hug"`. 3步后的Vocabulary和Corpus为: 

   ```
   Vocabulary: ["b", "h", "p", "##g", "##n", "##s", "##u", "##gs", "hu", "hug"]
   Corpus: ("hug", 10), ("p" "##u" "##g", 5), ("p" "##u" "##n", 12), ("b" "##u" "##n", 4), ("hu" "##gs", 5)
   ```

### Tokenization algorithm

不同于BPE, WordPiece只保存最终的Vocabulary, 而不是保存merges. 且WordPiece优先按Vocabulary中最长的subword进行分词.

```python
def encode_word(word):
    tokens = []
    while len(word) > 0:
        i = len(word)
        while i > 0 and word[:i] not in vocab:
            i -= 1
        if i == 0:
            return ["[UNK]"]
        tokens.append(word[:i])
        word = word[i:]
        if len(word) > 0:
            word = f"##{word}"
    return tokens

def tokenize(text):
    pre_tokenize_result = tokenizer._tokenizer.pre_tokenizer.pre_tokenize_str(text)
    pre_tokenized_text = [word for word, offset in pre_tokenize_result]
    encoded_words = [encode_word(word) for word in pre_tokenized_text]
    return sum(encoded_words, [])
```

对于例子中得到的Vocabulary, WordPiece的分词结果如下:

```
"hugs" -> ["hug", "##s"]
"bugs" -> ["b", "##ugs"] -> ["b", "##u, "##gs"]
```

## Unigram

[Unigram tokenization - Hugging Face NLP Course](https://huggingface.co/learn/nlp-course/en/chapter6/7?fw=pt)

### Training algorithm

不同于WordPiece和BPE逐步扩充 Vocabulary, Unigram的初始 Vocabulary 很大, 然后逐步删减.

有很多方法用于创建初始 Vocabulary, 例如: pre-tokenized words 中的最常见子串(most common substrings), 或者调用BPE,设置一个很大的vocabulary size.

1. 每一步, 计算给定的当前 Vocabulary 下, 整个 corpus 的 loss. 
2. 然后对于 vocabulary 中的每个 token, 计算当它从 vocabulary 移除后, 会导致 loss 增加多少. 
3. 找到使 loss 增加最少的tokens, 这些符号对语料库整体损失的影响较小，因此从某种意义上说，它们“不太需要”，是删除的最佳候选。

这是一个非常耗时的过程, 因此一般每次都删掉 10% 或 20% 的tokens. 此外为了保证所有的 word 都能被 tokenized, 所有的 base character 都不会被删除.

依然假设 Corpus 如下: 

```
("hug", 10), ("pug", 5), ("pun", 12), ("bun", 4), ("hugs", 5)
```

对于这个例子, 我们用所有的 substring 初始化 Vocabulary:

```
["h", "u", "g", "hu", "ug", "p", "pu", "n", "un", "b", "bu", "s", "hug", "gs", "ugs"]
```

怎么算 loss 呢, 暂且按下不表.

### Tokenization algorithm

Unigram model, 一种语言模型, 每个token与前文是独立的, 即 $P(x_i)=P(x_i|x_1, \dots. x_{i-1})$. 如果用一个 Unigram language model 来生成文本, 它将总是预测the most common token. 

一个 token 的概率 $P(x)$, 等于它在原始 corpus 中出现的频率除以所有 token 的频率的和.

下面是所有 词表中所有 substring 的频率: 

```
("h", 15) ("u", 36) ("g", 20) ("hu", 15) ("ug", 20) ("p", 17) ("pu", 17) ("n", 16)
("un", 16) ("b", 4) ("bu", 4) ("s", 5) ("hug", 15) ("gs", 5) ("ugs", 5)
```

所有 token 的频率的和为 210, "ug"的频率为 20, 那么 $P(\text{'ug'})=\frac{20}{210}$.

tokenize 一个 word 的  步骤如下: 

1. 找到所有可能的分割方式(segmentations), 例如, pug 可以分为, `["p", "u", "g"], ["pu", "g"], ["p", "ug"]`

2. 计算每个每种分割方式的概率, 因为每个token都是独立的, 因此这个概率就是各个token概率的乘积
   $$
   P([\text{'p'}, \text{'u'}, \text{'g'}]) = P(\text{'p'}) \times P(\text{'u'}) \times P(\text{'g'}) = \frac{5}{210} \times \frac{36}{210} \times \frac{20}{210} = 0.000389
   $$

   ```
   ["p", "u", "g"] : 0.000389
   ["p", "ug"] : 0.0022676
   ["pu", "g"] : 0.0022676
   ```

3. 选择概率最高的分词方式 (一般来说，使用尽可能少的 tokens 将具有最高的概率(因为每个 token 重复除以210)，这与我们直观地想要的相对应: 将一个单词分成尽可能少的tokens)

> 在这个例子中, 找到所有的 segmentation 并计算它们的概率是简单的. 但一般情况下, 这是有些难以实现的. 有一个处理该问题的经典算法, 叫*Viterbi algorithm*. 

### Back to training

loss 的计算就是, 用当前的Unigram model(与vocabulary有关)对corpus中的word进行tokenization, 得到对应的score, loss就是这些score的 negative log likelihood, 即corpus中所有word的 $-\log (\text{score})$ 的和.

对corpus中word的tokenization结果如下: 

```
"hug": ["hug"] (score 0.071428)
"pug": ["pu", "g"] (score 0.007710)
"pun": ["pu", "n"] (score 0.006168)
"bun": ["bu", "n"] (score 0.001451)
"hugs": ["hug", "s"] (score 0.001701)
```

则loss为: 

```
10 * (-log(0.071428)) + 5 * (-log(0.007710)) + 12 * (-log(0.006168)) + 4 * (-log(0.001451)) + 5 * (-log(0.001701)) = 169.8
```



> Unigram is a subword tokenization algorithm introduced in [Subword Regularization: Improving Neural Network Translation Models with Multiple Subword Candidates (Kudo, 2018)](https://arxiv.org/pdf/1804.10959.pdf). <u>In contrast to BPE or WordPiece, Unigram initializes its base vocabulary to a large number of symbols and progressively trims down each symbol to obtain a smaller vocabulary.</u> The base vocabulary could for instance correspond to all pre-tokenized words and the most common substrings. Unigram is not used directly for any of the models in the transformers, but it’s used in conjunction with SentencePiece.
>
> At each training step, the Unigram algorithm defines a loss (often defined as the log-likelihood) over the training data given the current vocabulary and a unigram language model. Then, for each symbol in the vocabulary, the algorithm computes how much the overall loss would increase if the symbol was to be removed from the vocabulary. Unigram then removes p (with p usually being 10% or 20%) percent of the symbols whose loss increase is the lowest, *i.e.* those symbols that least affect the overall loss over the training data. This process is repeated until the vocabulary has reached the desired size. The Unigram algorithm always keeps the base characters so that any word can be tokenized.
>
> Because Unigram is not based on merge rules (in contrast to BPE and WordPiece), the algorithm has several ways of tokenizing new text after training. As an example, if a trained Unigram tokenizer exhibits the vocabulary:
>
> ```
> ["b", "g", "h", "n", "p", "s", "u", "ug", "un", "hug"],
> ```
>
> `"hugs"` could be tokenized both as `["hug", "s"]`, `["h", "ug", "s"]` or `["h", "u", "g", "s"]`. So which one to choose? Unigram saves the probability of each token in the training corpus on top of saving the vocabulary so that the probability of each possible tokenization can be computed after training. The algorithm simply picks the most likely tokenization in practice, but also offers the possibility to sample a possible tokenization according to their probabilities.

## SentencePiece

[Summary of the tokenizers (huggingface.co)](https://huggingface.co/docs/transformers/en/tokenizer_summary#sentencepiece)

不是所有的语言都是用空格分隔, sentencePiece可以解决这个问题.

pre-tokenization: It considers the text as a sequence of Unicode characters, and replaces spaces with a special character, `▁`. 从这里也可以看出它不依赖空格做pre-tokenization, 事实上就算有空格, 他也会把空格处理掉.

> All tokenization algorithms described so far have the same problem: <u>It is assumed that the input text uses spaces to separate words. However, not all languages use spaces to separate words</u>. One possible solution is to use language specific pre-tokenizers, *e.g.* XLM uses a specific Chinese, Japanese, and Thai pre-tokenizer). To solve this problem more generally, [SentencePiece: A simple and language independent subword tokenizer and detokenizer for Neural Text Processing (Kudo et al., 2018)](https://arxiv.org/pdf/1808.06226.pdf) treats the input as a raw input stream, thus including the space in the set of characters to use. <u>It then uses the BPE or unigram algorithm to construct the appropriate vocabulary.</u>
>
> Used in conjunction with the Unigram algorithm (see [section 7](https://huggingface.co/course/chapter7/7)), it doesn’t even require a pre-tokenization step, which is very useful for languages where the space character is not used (like Chinese or Japanese).
>
> ```python
> from transformers import XLNetTokenizer
> 
> tokenizer = XLNetTokenizer.from_pretrained("xlnet/xlnet-base-cased")
> tokenizer.tokenize("Don't you love 🤗 Transformers? We sure do.")
> ["▁Don", "'", "t", "▁you", "▁love", "▁", "🤗", "▁", "Transform", "ers", "?", "▁We", "▁sure", "▁do", "."]
> ```
>
> The XLNetTokenizer uses SentencePiece for example, which is also why in the example earlier the `"▁"` character was included in the vocabulary. Decoding with SentencePiece is very easy since all tokens can just be concatenated and `"▁"` is replaced by a space.
>
> **All transformers models in the library that use SentencePiece use it in combination with unigram. Examples of models using SentencePiece are [ALBERT](https://huggingface.co/docs/transformers/en/model_doc/albert), [XLNet](https://huggingface.co/docs/transformers/en/model_doc/xlnet), [Marian](https://huggingface.co/docs/transformers/en/model_doc/marian), and [T5](https://huggingface.co/docs/transformers/en/model_doc/t5).**
