## Classificationç³»åˆ—

ğŸŒ°

æ··æ·†çŸ©é˜µ: 

### <img src="https://img2023.cnblogs.com/blog/3103884/202308/3103884-20230802143009246-252601572.png" style="zoom: 33%;" />

### ç²¾ç¡®ç‡ (Precision)
$$
\begin{equation}
\text{Precision}=\frac{TP}{TP+FP}
\end{equation}
$$
è¢«æ¨¡å‹é¢„æµ‹ä¸ºç±»åˆ«Cçš„æ ·æœ¬ä¸­, æœ‰å¤šå°‘æ˜¯æ­£ç¡®çš„.
### å¬å›ç‡ (Recall)
$$
\begin{equation}
\text{Recall}=\frac{TP}{TP+FN}
\end{equation}
$$
çœŸå®ç±»åˆ«ä¸ºCçš„æ ·æœ¬ä¸­, æœ‰å¤šå°‘è¢«æ¨¡å‹å¬å›.
### F1å€¼ (F1-score)
$$
\begin{equation}
    \begin{aligned}
        F_1&=\frac{2}{\text{Precision}^{-1}+\text{Recall}^{-1}}\\ \\
           &=2\frac{\text{Precision}\times\text{Recall}}{\text{Precision}+\text{Recall}}\\ \\
           &= \frac{2TP}{2TP+FP+FN}
    \end{aligned}
\end{equation}
$$
ç²¾ç¡®åº¦å’Œå¬å›ç‡çš„è°ƒå’Œå¹³å‡
### å®å¹³å‡ (Macro-average), å¾®å¹³å‡ (Micro-average), åŠ æƒå¹³å‡(Weighted average)
ä»¥ä¸Šè®¨è®ºçš„æ˜¯é’ˆå¯¹æŸä¸€ä¸ªç±»åˆ«çš„æƒ…å†µ, å¦‚æœè®¡ç®—å¤šç±»åˆ«åˆ†ç±»çš„ Precision, Recall å’Œ F1-score, åˆ™æœ‰ä»¥ä¸‹å‡ ç§æ–¹å¼: 
- å®å¹³å‡ï¼ˆMacro-averageï¼‰ï¼šé¦–å…ˆåˆ†åˆ«è®¡ç®—æ¯ä¸ªç±»åˆ«çš„ç²¾ç¡®åº¦ (æˆ–å¬å›ç‡, æˆ–F1)ï¼Œç„¶åå¯¹æ‰€æœ‰ç±»åˆ«çš„ç²¾ç¡®åº¦ï¼ˆæˆ–å¬å›ç‡, æˆ–F1ï¼‰å–å¹³å‡ã€‚è¿™ç§æ–¹å¼å‡å®šæ‰€æœ‰çš„ç±»åˆ«åŒç­‰é‡è¦ï¼Œä½†å¯èƒ½å—åˆ°å°ç±»åˆ«çš„å½±å“ã€‚
- å¾®å¹³å‡ï¼ˆMicro-averageï¼‰ï¼šå…ˆå…¨å±€è®¡ç®—å‡ºæ‰€æœ‰ç±»åˆ«çš„çœŸé˜³æ€§(TP)ã€å‡é˜³æ€§(FP)å’Œå‡é˜´æ€§(FN)æ•°é‡ï¼Œç„¶åç”¨è¿™äº›å€¼è®¡ç®—ç²¾ç¡®åº¦ (æˆ–å¬å›ç‡, æˆ–F1)ã€‚è¿™ç§æ–¹å¼æ›´å…³æ³¨å¤§ç±»åˆ«çš„æ€§èƒ½ã€‚
- åŠ æƒå¹³å‡ï¼ˆWeighted averageï¼‰ï¼šè¿™ä¹Ÿæ˜¯å…ˆåˆ†åˆ«è®¡ç®—æ¯ä¸ªç±»åˆ«çš„ç²¾ç¡®åº¦ (æˆ–å¬å›ç‡, æˆ–F1)ï¼Œä½†æ˜¯åœ¨æ±‚å¹³å‡æ—¶ä¼šæ ¹æ®æ¯ä¸ªç±»åˆ«çš„æ ·æœ¬æ•°é‡ç»™äºˆä¸åŒçš„æƒé‡ã€‚è¿™ç§æ–¹å¼è¯•å›¾åœ¨å®å¹³å‡å’Œå¾®å¹³å‡ä¹‹é—´æ‰¾åˆ°å¹³è¡¡ã€‚

## Rougeç³»åˆ—

[å‚è€ƒåŸæ–‡](https://medium.com/nlplanet/two-minutes-nlp-learn-the-rouge-metric-by-examples-f179cc285499)

ğŸŒ° 

Consider the reference $R$ and the candidate $C$:

- $R$: The cat is on the mat.
- $C$: The cat and the dog.

### ROUGE-N

ROUGE-N measures the number of matching [n-grams](https://en.wikipedia.org/wiki/N-gram) between the $C$ and $R$.

ROUGE-1 precision, $C$ å’Œ $R$ å…¬å…±çš„ unigrams (â€œtheâ€, â€œcatâ€, and â€œtheâ€)çš„ä¸ªæ•°, æ¯”ä¸Š $C$ ä¸­ unigrams çš„æ•°é‡.

> ROUGE-1 precision = 3/5 = 0.6

ROUGE-1 recall, $C$ å’Œ $R$ å…¬å…±çš„ unigrams (â€œtheâ€, â€œcatâ€, and â€œtheâ€)çš„ä¸ªæ•°, æ¯”ä¸Š $R$ ä¸­ unigrams çš„æ•°é‡.

> ROUGE-1 recall = 3/6 = 0.5

ROUGE-1 F1-score, can be directly obtained from the ROUGE-1 precision and recall using the standard F1-score formula.

> ROUGE-1 F1-score = 2 * (precision * recall) / (precision + recall) = 0.54

ROUGE-2 åŒç†, ä¾‹å­ä¸­çš„å…¬å…±çš„ 2-gram åªæœ‰â€œthe catâ€

> ROUGE-2 precision = 1/4 = 0.25
>
> ROUGE-2 recall = 1/5 = 0.20
>
> ROUGE-2 F1-score = 2 * (precision * recall) / (precision + recall) = 0.22

### ROUGE-L

ROUGE-L is based on the [longest common subsequence (LCS)](https://en.wikipedia.org/wiki/Longest_common_subsequence_problem) between $C$ and $R$, i.e. the longest sequence of words (not necessarily consecutive, but still in order) that is shared between both. A longer shared sequence should indicate more similarity between the two sequences.

ä¾‹å­ä¸­, $C$ å’Œ $R$ çš„ LCS ä¸º â€œthe cat theâ€

ROUGE-L precision, LCSçš„é•¿åº¦æ¯”ä¸Š $C$ ä¸­ unigrams çš„æ•°é‡.

> ROUGE-L precision = 3/5 = 0.6

ROUGE-L precision, LCSçš„é•¿åº¦æ¯”ä¸Š $R$ ä¸­ unigrams çš„æ•°é‡.

> ROUGE-L recall = 3/6 = 0.5

Therefore, the F1-score is:

> ROUGE-L F1-score = 2 * (precision * recall) / (precision + recall) = 0.55

## Rankingç³»åˆ—

ğŸŒ° 

æœ‰ä¸€ä¸ª query åˆ—è¡¨ $\mathcal Q$, ç”¨ $|\mathcal Q|$ è¡¨ç¤º query ä¸ªæ•°. $q_i$ è¡¨ç¤ºå…¶ä¸­ ç¬¬ $i$ ä¸ª query.

æœ‰ä¸€ä¸ª document åº“ $\mathcal D$, ç”¨ $|\mathcal D|$ è¡¨ç¤º document ä¸ªæ•°. $d_i$ è¡¨ç¤ºå…¶ä¸­ç¬¬ $i$ ä¸ª document.

$\text{goldDoc}(q)$ è¡¨ç¤ºä¸€ä¸ª $q\in \mathcal Q$, æ‰€å¯¹åº”çš„gold document.

$\text{retrDoc}(q, k)$ è¡¨ç¤ºä¸€ä¸ª $q\in \mathcal Q$, ç”¨ Retriever æ£€ç´¢å›çš„ $k$ ä¸ª document.

### Recall@k

$$
\text{Recall}@k = \frac{1}{|\mathcal Q|}\sum_q^\mathcal Q {\frac{|\text{goldDoc}(q)\cap \text{retrDoc}(q, k)|}{|\text{goldDoc}(q)|}}
$$



### Precision@k

$$
\text{Precision}@k=\frac{1}{|\mathcal Q|}\sum_q^\mathcal Q {\frac{|\text{goldDoc}(q)\cap \text{retrDoc}(q, k)|}{|\text{retrDoc}(q)|}}=\frac{1}{|\mathcal Q|}\sum_q^\mathcal Q {\frac{|\text{goldDoc}(q)\cap \text{retrDoc}(q, k)|}{k}}
$$

### AP(Average Precision)

$$
\text{AP}_q=\frac{1}{|\text{goldDoc}(q)|}\sum_{k=1}^{|\mathcal D|}{\text{Precision}_q@k \times \mathbb I(q, d_k)}
$$

å…¶ä¸­, ä»…å½“ç¬¬ $k$ ä¸ª document $d_k$ ä¸ query $q$ ç›¸å…³æ—¶, $\mathbb I(q, d_k)=1$, å¦åˆ™ $\mathbb I(q, d_k)=0$

### MAP(Mean Average Precision)

$$
\text{MAP} = \frac{1}{|\mathcal Q|}\sum_q^\mathcal Q{AP_q}
$$

### NDGC(Normalized Discounted Cumulative Gain)

[å‚è€ƒ](https://developer.aliyun.com/article/1361549), NDGCä¸åªè€ƒè™‘æ£€ç´¢å›çš„Docçš„æ•°é‡, è¿˜å…³æ³¨Docçš„é¡ºåº

ä¾‹å¦‚, $\text{goldDoc}(q)$ æœ‰4ä¸ª document, å…¶åˆ†æ•°åˆ†åˆ«ä¸º3, 2, 1, 0.
$$
\text{DGC}_q@k=\sum_{i=1}^k{\frac{}{}}
$$




### MRR

ä¹Ÿæ˜¯ä¸€ä¸ªè€ƒè™‘é¡ºåºçš„æŒ‡æ ‡.
$$
\text{MRR} = \frac{1}{|\mathcal Q|}\sum_q^\mathcal Q{\frac{1}{\text{rank}_q}}
$$
å…¶ä¸­, $\text{rank}_q$ è¡¨ç¤ºé’ˆå¯¹ query $q$, Retriever å¯¹ $\mathcal D$ æ’åºå, ç¬¬ä¸€ä¸ª gold document æ‰€åœ¨çš„ä½ç½®.
