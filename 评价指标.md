## Classification系列

🌰

混淆矩阵: 

### <img src="https://img2023.cnblogs.com/blog/3103884/202308/3103884-20230802143009246-252601572.png" style="zoom: 33%;" />

### 精确率 (Precision)
$$
\begin{equation}
\text{Precision}=\frac{TP}{TP+FP}
\end{equation}
$$
被模型预测为类别C的样本中, 有多少是正确的.
### 召回率 (Recall)
$$
\begin{equation}
\text{Recall}=\frac{TP}{TP+FN}
\end{equation}
$$
真实类别为C的样本中, 有多少被模型召回.
### F1值 (F1-score)
$$
\begin{equation}
    \begin{aligned}
        F_1&=\frac{2}{\text{Precision}^{-1}+\text{Recall}^{-1}}\\ \\
           &=2\frac{\text{Precision}\times\text{Recall}}{\text{Precision}+\text{Recall}}\\ \\
           &= \frac{2TP}{2TP+FP+FN}
    \end{aligned}
\end{equation}
$$
精确度和召回率的调和平均
### 宏平均 (Macro-average), 微平均 (Micro-average), 加权平均(Weighted average)
以上讨论的是针对某一个类别的情况, 如果计算多类别分类的 Precision, Recall 和 F1-score, 则有以下几种方式: 
- 宏平均（Macro-average）：首先分别计算每个类别的精确度 (或召回率, 或F1)，然后对所有类别的精确度（或召回率, 或F1）取平均。这种方式假定所有的类别同等重要，但可能受到小类别的影响。
- 微平均（Micro-average）：先全局计算出所有类别的真阳性(TP)、假阳性(FP)和假阴性(FN)数量，然后用这些值计算精确度 (或召回率, 或F1)。这种方式更关注大类别的性能。
- 加权平均（Weighted average）：这也是先分别计算每个类别的精确度 (或召回率, 或F1)，但是在求平均时会根据每个类别的样本数量给予不同的权重。这种方式试图在宏平均和微平均之间找到平衡。

## Rouge系列

[参考原文](https://medium.com/nlplanet/two-minutes-nlp-learn-the-rouge-metric-by-examples-f179cc285499)

🌰 

Consider the reference $R$ and the candidate $C$:

- $R$: The cat is on the mat.
- $C$: The cat and the dog.

### ROUGE-N

ROUGE-N measures the number of matching [n-grams](https://en.wikipedia.org/wiki/N-gram) between the $C$ and $R$.

ROUGE-1 precision, $C$ 和 $R$ 公共的 unigrams (“the”, “cat”, and “the”)的个数, 比上 $C$ 中 unigrams 的数量.

> ROUGE-1 precision = 3/5 = 0.6

ROUGE-1 recall, $C$ 和 $R$ 公共的 unigrams (“the”, “cat”, and “the”)的个数, 比上 $R$ 中 unigrams 的数量.

> ROUGE-1 recall = 3/6 = 0.5

ROUGE-1 F1-score, can be directly obtained from the ROUGE-1 precision and recall using the standard F1-score formula.

> ROUGE-1 F1-score = 2 * (precision * recall) / (precision + recall) = 0.54

ROUGE-2 同理, 例子中的公共的 2-gram 只有“the cat”

> ROUGE-2 precision = 1/4 = 0.25
>
> ROUGE-2 recall = 1/5 = 0.20
>
> ROUGE-2 F1-score = 2 * (precision * recall) / (precision + recall) = 0.22

### ROUGE-L

ROUGE-L is based on the [longest common subsequence (LCS)](https://en.wikipedia.org/wiki/Longest_common_subsequence_problem) between $C$ and $R$, i.e. the longest sequence of words (not necessarily consecutive, but still in order) that is shared between both. A longer shared sequence should indicate more similarity between the two sequences.

例子中, $C$ 和 $R$ 的 LCS 为 “the cat the”

ROUGE-L precision, LCS的长度比上 $C$ 中 unigrams 的数量.

> ROUGE-L precision = 3/5 = 0.6

ROUGE-L precision, LCS的长度比上 $R$ 中 unigrams 的数量.

> ROUGE-L recall = 3/6 = 0.5

Therefore, the F1-score is:

> ROUGE-L F1-score = 2 * (precision * recall) / (precision + recall) = 0.55

## Ranking系列

🌰 

有一个 query 列表 $\mathcal Q$, 用 $|\mathcal Q|$ 表示 query 个数. $q_i$ 表示其中 第 $i$ 个 query.

有一个 document 库 $\mathcal D$, 用 $|\mathcal D|$ 表示 document 个数. $d_i$ 表示其中第 $i$ 个 document.

$\text{goldDoc}(q)$ 表示一个 $q\in \mathcal Q$, 所对应的gold document.

$\text{retrDoc}(q, k)$ 表示一个 $q\in \mathcal Q$, 用 Retriever 检索回的 $k$ 个 document.

### Recall@k

$$
\text{Recall}@k = \frac{1}{|\mathcal Q|}\sum_q^\mathcal Q {\frac{|\text{goldDoc}(q)\cap \text{retrDoc}(q, k)|}{|\text{goldDoc}(q)|}}
$$



### Precision@k

$$
\text{Precision}@k=\frac{1}{|\mathcal Q|}\sum_q^\mathcal Q {\frac{|\text{goldDoc}(q)\cap \text{retrDoc}(q, k)|}{|\text{retrDoc}(q)|}}=\frac{1}{|\mathcal Q|}\sum_q^\mathcal Q {\frac{|\text{goldDoc}(q)\cap \text{retrDoc}(q, k)|}{k}}
$$

### AP(Average Precision)

$$
\text{AP}_q=\frac{1}{|\text{goldDoc}(q)|}\sum_{k=1}^{|\mathcal D|}{\text{Precision}_q@k \times \mathbb I(q, d_k)}
$$

其中, 仅当第 $k$ 个 document $d_k$ 与 query $q$ 相关时, $\mathbb I(q, d_k)=1$, 否则 $\mathbb I(q, d_k)=0$

### MAP(Mean Average Precision)

$$
\text{MAP} = \frac{1}{|\mathcal Q|}\sum_q^\mathcal Q{AP_q}
$$

### NDGC(Normalized Discounted Cumulative Gain)

[参考](https://developer.aliyun.com/article/1361549), NDGC不只考虑检索回的Doc的数量, 还关注Doc的顺序

例如, $\text{goldDoc}(q)$ 有4个 document, 其分数分别为3, 2, 1, 0.
$$
\text{DGC}_q@k=\sum_{i=1}^k{\frac{}{}}
$$




### MRR

也是一个考虑顺序的指标.
$$
\text{MRR} = \frac{1}{|\mathcal Q|}\sum_q^\mathcal Q{\frac{1}{\text{rank}_q}}
$$
其中, $\text{rank}_q$ 表示针对 query $q$, Retriever 对 $\mathcal D$ 排序后, 第一个 gold document 所在的位置.
